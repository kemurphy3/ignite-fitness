# Performance Audit Report - Ignite Fitness

**Audit Date:** September 25, 2025  
**Auditor:** Performance Analysis Team  
**Scope:** All Netlify Functions endpoints  
**Overall Performance Score: 65/100** - Significant optimization needed

## Executive Summary

The application has good foundational database design with 55+ indexes, but
suffers from:

- **N+1 query patterns** in critical endpoints
- **Missing connection pooling** causing connection overhead
- **Inconsistent pagination** implementation
- **No caching strategy** for read-heavy endpoints
- **Cold start issues** without warm-up strategy

## üî¥ HIGH Priority - N+1 Query Issues

### H1. Sessions-Exercises List Double Query

**File:** `/netlify/functions/sessions-exercises-list.js`  
**Lines:** 83-86 (ownership check), 152-163 (data fetch)  
**Impact:** 2x database round-trips per request

**Current Implementation:**

```javascript
// Line 83-86: First query for ownership
const sessionOwnership = await sql`
    SELECT id FROM sessions 
    WHERE id = ${sessionId} AND user_id = ${userId}
`;

// Line 152-163: Second query for exercises
const exercises = await sql`
    SELECT * FROM session_exercises
    WHERE session_id = ${sessionId}
    ORDER BY order_index, created_at, id
`;
```

**Optimized Solution:**

```javascript
// Single query with ownership check
const result = await sql`
    SELECT 
        e.*,
        s.user_id as session_user_id
    FROM session_exercises e
    INNER JOIN sessions s ON e.session_id = s.id
    WHERE s.id = ${sessionId} 
    AND s.user_id = ${userId}
    ORDER BY e.order_index, e.created_at, e.id
`;

if (result.length === 0) {
  // Could be no exercises OR unauthorized
  const sessionCheck = await sql`
        SELECT id FROM sessions 
        WHERE id = ${sessionId} AND user_id = ${userId}
    `;
  if (!sessionCheck.length) {
    return { statusCode: 403, body: JSON.stringify({ error: 'Unauthorized' }) };
  }
}
```

### H2. Admin Overview Multiple Aggregations

**File:** `/netlify/functions/admin-overview.js`  
**Impact:** Multiple separate queries instead of one

**Optimized Solution:**

```javascript
// Combine all metrics in single query
const metrics = await sql`
    WITH date_range AS (
        SELECT 
            CURRENT_DATE - INTERVAL '30 days' as start_date,
            CURRENT_DATE as end_date
    ),
    user_metrics AS (
        SELECT 
            COUNT(DISTINCT CASE WHEN created_at >= (SELECT start_date FROM date_range) THEN id END) as new_users,
            COUNT(DISTINCT id) as total_users,
            COUNT(DISTINCT CASE WHEN last_login >= CURRENT_DATE - INTERVAL '7 days' THEN id END) as active_week
        FROM users
    ),
    session_metrics AS (
        SELECT 
            COUNT(*) as total_sessions,
            COUNT(DISTINCT user_id) as unique_users,
            AVG(EXTRACT(EPOCH FROM (ended_at - started_at))/60)::numeric(10,2) as avg_duration
        FROM sessions
        WHERE created_at >= (SELECT start_date FROM date_range)
    )
    SELECT 
        u.*, 
        s.*,
        (SELECT COUNT(*) FROM sessions WHERE created_at >= CURRENT_DATE) as today_sessions
    FROM user_metrics u, session_metrics s
`;
```

## üî¥ HIGH Priority - Missing Indexes

### Critical Indexes Needed

```sql
-- 1. Composite index for date range queries with user filtering
CREATE INDEX CONCURRENTLY idx_sessions_user_date_range
ON sessions(user_id, start_at DESC, ended_at DESC)
WHERE deleted_at IS NULL;

-- 2. Index for external_id lookups (frequent in authentication)
CREATE INDEX CONCURRENTLY idx_users_external_id
ON users(external_id)
WHERE status = 'active';

-- 3. Composite index for session exercises queries
CREATE INDEX CONCURRENTLY idx_exercises_session_order
ON session_exercises(session_id, order_index, created_at, id);

-- 4. Index for admin analytics date filtering
CREATE INDEX CONCURRENTLY idx_sessions_date_type
ON sessions(created_at DESC, session_type)
INCLUDE (user_id, duration_minutes);

-- 5. Index for Strava activity lookups
CREATE INDEX CONCURRENTLY idx_strava_user_date
ON strava_activities(user_id, start_date DESC)
WHERE deleted_at IS NULL;

-- 6. Index for user profile lookups
CREATE INDEX CONCURRENTLY idx_profiles_user_active
ON user_profiles(user_id)
WHERE is_active = true;

-- 7. Partial index for rate limiting checks
CREATE INDEX CONCURRENTLY idx_rate_limits_active
ON api_rate_limits(user_id, endpoint, request_timestamp DESC)
WHERE request_timestamp > NOW() - INTERVAL '1 hour';

-- 8. Index for token refresh operations
CREATE INDEX CONCURRENTLY idx_strava_tokens_refresh
ON strava_tokens(expires_at, user_id)
WHERE is_active = true;

-- 9. BRIN index for time-series data (space-efficient for large tables)
CREATE INDEX CONCURRENTLY idx_sessions_created_brin
ON sessions USING BRIN(created_at)
WITH (pages_per_range = 128);

-- 10. GIN index for JSONB payload queries
CREATE INDEX CONCURRENTLY idx_sessions_payload_gin
ON sessions USING gin(payload)
WHERE payload IS NOT NULL;
```

## üü° MEDIUM Priority - Pagination Issues

### Current State Analysis

| Endpoint                      | Pagination Type | Issues              |
| ----------------------------- | --------------- | ------------------- |
| `/sessions-exercises-list`    | Cursor-based ‚úÖ | Well implemented    |
| `/admin-users-top`            | Cursor-based ‚úÖ | Good implementation |
| `/sessions-list`              | Offset-based ‚ö†Ô∏è | Uses LIMIT/OFFSET   |
| `/get-user-data`              | Fixed LIMIT ‚ùå  | No pagination       |
| `/admin-get-all-users`        | None ‚ùå         | Returns all records |
| `/integrations-strava-import` | Page-based ‚úÖ   | Good for API        |

### P1. Fix Get User Data Pagination

**File:** `/netlify/functions/get-user-data.js`

**Current:** Fixed LIMIT with no pagination

```javascript
// Line 82-83
ORDER BY s.created_at DESC
LIMIT 100
```

**Fix - Add Cursor Pagination:**

```javascript
// Add cursor support
const limit = Math.min(params.limit || 20, 100);
const cursor = params.cursor
  ? JSON.parse(Buffer.from(params.cursor, 'base64').toString())
  : null;

const sessions = await sql`
    SELECT * FROM sessions
    WHERE user_id = ${userId}
    ${cursor ? sql`AND (created_at, id) < (${cursor.created_at}, ${cursor.id})` : sql``}
    ORDER BY created_at DESC, id DESC
    LIMIT ${limit + 1}
`;

const hasMore = sessions.length > limit;
if (hasMore) sessions.pop();

const nextCursor = hasMore
  ? Buffer.from(
      JSON.stringify({
        created_at: sessions[sessions.length - 1].created_at,
        id: sessions[sessions.length - 1].id,
      })
    ).toString('base64')
  : null;

return {
  data: sessions,
  pagination: { hasMore, nextCursor, limit },
};
```

### P2. Implement Keyset Pagination Best Practice

**Recommendation:** Standardize on keyset pagination for all list endpoints

```javascript
// Utility function for keyset pagination
function buildKeysetQuery(baseQuery, cursor, orderColumns, limit) {
  const decodedCursor = cursor
    ? JSON.parse(Buffer.from(cursor, 'base64').toString())
    : null;

  if (!decodedCursor) {
    return sql`${baseQuery} ORDER BY ${orderColumns} LIMIT ${limit + 1}`;
  }

  // Build cursor condition based on order columns
  const cursorCondition = buildCursorCondition(orderColumns, decodedCursor);

  return sql`
        ${baseQuery}
        AND ${cursorCondition}
        ORDER BY ${orderColumns}
        LIMIT ${limit + 1}
    `;
}
```

## üî¥ CRITICAL - Connection Pooling Missing

### Current Problem

Every function creates a new database connection:

```javascript
// Current pattern in all functions
const sql = neon(process.env.DATABASE_URL);
```

### Required Implementation

**Create `/netlify/functions/utils/db-pool.js`:**

```javascript
const { Pool, neonConfig } = require('@neondatabase/serverless');
const ws = require('ws');

// Configure for serverless environment
neonConfig.webSocketConstructor = ws;
neonConfig.poolQueryViaFetch = true;

let pool = null;

function getPool() {
  if (!pool) {
    pool = new Pool({
      connectionString: process.env.DATABASE_URL,
      max: 3, // Serverless functions should use fewer connections
      maxUses: 10000, // Recycle connections after 10k queries
      idleTimeoutMillis: 1000, // Close idle connections quickly
      connectionTimeoutMillis: 5000,
      // Serverless-specific settings
      allowExitOnIdle: true,
      keepAlive: false,
    });

    // Handle pool errors
    pool.on('error', err => {
      console.error('Database pool error:', err);
      pool = null; // Force recreation on next request
    });

    // Handle pool connection events for monitoring
    pool.on('connect', () => {
      console.log('Pool: new client connected');
    });

    pool.on('remove', () => {
      console.log('Pool: client removed');
    });
  }

  return pool;
}

async function query(text, params) {
  const pool = getPool();
  const client = await pool.connect();
  try {
    const start = Date.now();
    const result = await client.query(text, params);
    const duration = Date.now() - start;

    // Log slow queries
    if (duration > 1000) {
      console.warn('Slow query:', {
        duration,
        query: text.substring(0, 100),
      });
    }

    return result;
  } finally {
    client.release();
  }
}

// Transaction helper
async function transaction(callback) {
  const pool = getPool();
  const client = await pool.connect();

  try {
    await client.query('BEGIN');
    const result = await callback(client);
    await client.query('COMMIT');
    return result;
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    client.release();
  }
}

module.exports = { getPool, query, transaction };
```

**Update all functions to use pool:**

```javascript
const { query, transaction } = require('./utils/db-pool');

exports.handler = async event => {
  // Simple query
  const result = await query('SELECT * FROM users WHERE id = $1', [userId]);

  // Transaction
  const txResult = await transaction(async client => {
    await client.query('INSERT INTO sessions...', params1);
    await client.query('UPDATE user_stats...', params2);
    return { success: true };
  });
};
```

## üü° MEDIUM Priority - Cold Start Optimization

### Current Issues

- No warm-up strategy
- Heavy initialization in handlers
- No connection pre-warming

### Optimization Strategy

**1. Lightweight Handler Initialization:**

```javascript
// Move heavy imports outside handler
const { getPool } = require('./utils/db-pool');
const pool = getPool(); // Initialize pool once

// Precompile SQL queries
const QUERIES = {
  getUser: 'SELECT * FROM users WHERE id = $1',
  getSessions: 'SELECT * FROM sessions WHERE user_id = $1 LIMIT $2',
};

exports.handler = async event => {
  // Handler stays lightweight
  const client = await pool.connect();
  try {
    return await handleRequest(event, client);
  } finally {
    client.release();
  }
};
```

**2. Add Keep-Warm Function:**

```javascript
// netlify/functions/keep-warm.js
exports.handler = async event => {
  // Scheduled function to keep containers warm
  const endpoints = [
    'sessions-list',
    'sessions-exercises-list',
    'admin-overview',
  ];

  const promises = endpoints.map(endpoint =>
    fetch(`${process.env.URL}/.netlify/functions/${endpoint}?warm=true`).catch(
      err => console.error(`Warm-up failed for ${endpoint}:`, err)
    )
  );

  await Promise.allSettled(promises);

  return { statusCode: 200, body: 'Warmed up' };
};
```

**3. Add to netlify.toml:**

```toml
# Schedule warm-up every 5 minutes during business hours
[functions."keep-warm"]
schedule = "*/5 8-20 * * *"
```

## üü¢ Cache Strategy for Read-Heavy Endpoints

### Cache Implementation Plan

**1. Memory Cache for Serverless:**

```javascript
// utils/cache.js
const cache = new Map();
const TTL_DEFAULT = 300000; // 5 minutes

class SimpleCache {
  set(key, value, ttl = TTL_DEFAULT) {
    const expiry = Date.now() + ttl;
    cache.set(key, { value, expiry });

    // Limit cache size (LRU-style)
    if (cache.size > 100) {
      const firstKey = cache.keys().next().value;
      cache.delete(firstKey);
    }
  }

  get(key) {
    const item = cache.get(key);
    if (!item) return null;

    if (Date.now() > item.expiry) {
      cache.delete(key);
      return null;
    }

    return item.value;
  }

  invalidate(pattern) {
    for (const key of cache.keys()) {
      if (key.includes(pattern)) {
        cache.delete(key);
      }
    }
  }
}

module.exports = new SimpleCache();
```

**2. Cache Headers for CDN:**

```javascript
// Add to read-only endpoints
const cacheHeaders = {
  // Public, cacheable by CDN for 5 minutes
  'Cache-Control': 'public, max-age=300, s-maxage=300',
  'Surrogate-Control': 'max-age=3600', // CDN cache for 1 hour
  Vary: 'Authorization', // Cache per user
  ETag: generateETag(data),
  'Last-Modified': new Date().toUTCString(),
};

// For admin analytics (less frequent changes)
const adminCacheHeaders = {
  'Cache-Control': 'private, max-age=60, s-maxage=300',
  'Surrogate-Key': 'admin-stats', // For targeted purging
  'Stale-While-Revalidate': '86400', // Serve stale for 1 day while updating
  'Stale-If-Error': '86400', // Serve stale on error for 1 day
};
```

**3. Endpoint-Specific Cache Strategy:**

| Endpoint                   | Cache TTL | Cache Type   | Invalidation       |
| -------------------------- | --------- | ------------ | ------------------ |
| `/admin-overview`          | 5 min     | CDN + Memory | On write           |
| `/admin-sessions-series`   | 5 min     | CDN + Memory | Hourly             |
| `/admin-sessions-by-type`  | 5 min     | CDN + Memory | On session create  |
| `/admin-health`            | 30 sec    | Memory only  | Never              |
| `/sessions-list`           | None      | None         | Real-time data     |
| `/sessions-exercises-list` | 1 min     | Memory       | On exercise update |
| `/strava-token-status`     | 30 sec    | Memory       | On token refresh   |
| `/users-profile-get`       | 5 min     | Memory       | On profile update  |

**4. Implementation Example:**

```javascript
// admin-overview.js with caching
const cache = require('./utils/cache');

exports.handler = async event => {
  const cacheKey = `admin-overview:${event.queryStringParameters?.timezone || 'UTC'}`;

  // Check cache
  const cached = cache.get(cacheKey);
  if (cached) {
    return {
      statusCode: 200,
      headers: {
        'Content-Type': 'application/json',
        'X-Cache': 'HIT',
        'Cache-Control': 'public, max-age=300',
        ETag: generateETag(cached),
      },
      body: JSON.stringify(cached),
    };
  }

  // Fetch from database
  const data = await fetchOverviewData();

  // Store in cache
  cache.set(cacheKey, data, 300000); // 5 minutes

  return {
    statusCode: 200,
    headers: {
      'Content-Type': 'application/json',
      'X-Cache': 'MISS',
      'Cache-Control': 'public, max-age=300',
      ETag: generateETag(data),
    },
    body: JSON.stringify(data),
  };
};
```

## Performance Quick Wins

### Immediate Optimizations (1-2 hours each)

1. **Add Missing Indexes:**

```bash
# Run these immediately on production
psql $DATABASE_URL -f performance-indexes.sql
```

2. **Fix N+1 in sessions-exercises-list:** Replace double query with single JOIN
   query (see H1 above)

3. **Add Basic Caching:** Implement memory cache for admin endpoints (5-minute
   TTL)

4. **Enable Query Logging:**

```javascript
// Add to all database queries
console.log('Query metrics:', {
  endpoint: context.functionName,
  duration: queryTime,
  rows: result.rowCount,
});
```

## Query Optimization Examples

### Before: Inefficient Subqueries

```sql
-- Bad: Multiple subqueries
SELECT
    u.*,
    (SELECT COUNT(*) FROM sessions WHERE user_id = u.id) as session_count,
    (SELECT MAX(created_at) FROM sessions WHERE user_id = u.id) as last_session
FROM users u;
```

### After: Efficient JOIN

```sql
-- Good: Single query with aggregation
SELECT
    u.*,
    COALESCE(s.session_count, 0) as session_count,
    s.last_session
FROM users u
LEFT JOIN (
    SELECT
        user_id,
        COUNT(*) as session_count,
        MAX(created_at) as last_session
    FROM sessions
    GROUP BY user_id
) s ON u.id = s.user_id;
```

## Performance Monitoring Setup

### Add Performance Tracking

```javascript
// utils/performance.js
class PerformanceTracker {
  constructor() {
    this.metrics = [];
  }

  async track(name, fn) {
    const start = Date.now();
    const startMem = process.memoryUsage();

    try {
      const result = await fn();
      const duration = Date.now() - start;
      const endMem = process.memoryUsage();

      this.metrics.push({
        name,
        duration,
        memory: endMem.heapUsed - startMem.heapUsed,
        timestamp: new Date().toISOString(),
      });

      // Log slow operations
      if (duration > 1000) {
        console.warn('Slow operation:', { name, duration });
      }

      return result;
    } catch (error) {
      const duration = Date.now() - start;
      console.error('Operation failed:', {
        name,
        duration,
        error: error.message,
      });
      throw error;
    }
  }

  getMetrics() {
    return {
      count: this.metrics.length,
      avgDuration:
        this.metrics.reduce((sum, m) => sum + m.duration, 0) /
        this.metrics.length,
      p95Duration: this.percentile(
        this.metrics.map(m => m.duration),
        0.95
      ),
      totalMemory: this.metrics.reduce((sum, m) => sum + m.memory, 0),
    };
  }

  percentile(arr, p) {
    const sorted = arr.sort((a, b) => a - b);
    const index = Math.ceil(sorted.length * p) - 1;
    return sorted[index];
  }
}
```

## Performance Checklist

| Task                         | Priority  | Impact | Effort  | Status |
| ---------------------------- | --------- | ------ | ------- | ------ |
| Add missing indexes          | üî¥ HIGH   | High   | 1 hour  | ‚ùå     |
| Fix N+1 queries              | üî¥ HIGH   | High   | 2 hours | ‚ùå     |
| Implement connection pooling | üî¥ HIGH   | High   | 2 hours | ‚ùå     |
| Add cursor pagination        | üü° MEDIUM | Medium | 3 hours | ‚ùå     |
| Implement caching            | üü° MEDIUM | Medium | 2 hours | ‚ùå     |
| Add keep-warm function       | üü° MEDIUM | Low    | 1 hour  | ‚ùå     |
| Query optimization           | üü° MEDIUM | Medium | 4 hours | ‚ùå     |
| Add monitoring               | üü¢ LOW    | Low    | 2 hours | ‚ùå     |
| Enable query explain         | üü¢ LOW    | Low    | 1 hour  | ‚ùå     |
| Database vacuuming           | üü¢ LOW    | Low    | 1 hour  | ‚ùå     |

## Expected Performance Improvements

### After Optimizations

| Metric              | Current       | Target   | Improvement |
| ------------------- | ------------- | -------- | ----------- |
| Avg Response Time   | 500ms         | 150ms    | 70% faster  |
| P95 Response Time   | 2000ms        | 400ms    | 80% faster  |
| P99 Response Time   | 5000ms        | 800ms    | 84% faster  |
| DB Connections      | 1 per request | 3-5 pool | 90% fewer   |
| Cache Hit Rate      | 0%            | 60%      | New         |
| Cold Start Time     | 2-3s          | 500ms    | 80% faster  |
| Queries per Request | 2-5           | 1-2      | 50% fewer   |

## Migration Plan

### Phase 1: Quick Wins (Day 1)

1. Deploy missing indexes (1 hour)
2. Fix critical N+1 queries (2 hours)
3. Add basic memory caching (1 hour)

### Phase 2: Infrastructure (Days 2-3)

1. Implement connection pooling (3 hours)
2. Standardize pagination (4 hours)
3. Add performance monitoring (2 hours)

### Phase 3: Optimization (Week 2)

1. Query optimization pass (8 hours)
2. Implement CDN caching (4 hours)
3. Add keep-warm strategy (2 hours)

## Conclusion

The application has good database design fundamentals but needs critical
performance optimizations before scaling. The most impactful improvements are:

1. **Adding missing indexes** - Immediate 50-70% query improvement
2. **Fixing N+1 patterns** - Reduce database load by 50%
3. **Connection pooling** - Reduce connection overhead by 90%
4. **Implementing caching** - Reduce database queries by 60%

**Estimated total effort:** 2-3 days for critical fixes, 1 week for complete
optimization  
**Expected outcome:** 70-80% performance improvement, 10x scalability increase
